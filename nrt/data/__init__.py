# Copyright (C) 2022 European Union (Joint Research Centre)
#
# Licensed under the EUPL, Version 1.2 or - as soon they will be approved by
# the European Commission - subsequent versions of the EUPL (the "Licence");
# You may not use this work except in compliance with the Licence.
# You may obtain a copy of the Licence at:
#
#   https://joinup.ec.europa.eu/software/page/eupl
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Licence is distributed on an "AS IS" basis,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the Licence for the specific language governing permissions and
# limitations under the Licence.

import os
import json

import xarray as xr
import rasterio
import numpy as np
import numba


data_dir = os.path.abspath(os.path.dirname(__file__))


def _load(f):
    """Load a ncdf file located in the data directory as a xarray Dataset

    Args:
        f (str): File basename

    Return:
        xarray.Dataset: The Dataset
    """
    xr_dataset = xr.open_dataset(os.path.join(data_dir, f))
    return xr_dataset


def romania_10m():
    """Sentinel 2 datacube of a small forested area in Romania at 10 m resolution

    Examples:
        >>> from nrt import data

        >>> s2_cube = data.romania_10m()
        >>> # Compute NDVI
        >>> s2_cube['ndvi'] = (s2_cube.B8 - s2_cube.B4) / (s2_cube.B8 + s2_cube.B4)
        >>> # Filter clouds
        >>> s2_cube = s2_cube.where(s2_cube.SCL.isin([4,5,7]))
    """
    return _load('sentinel2_cube_subset_romania_10m.nc')


def romania_20m():
    """Sentinel 2 datacube of a small forested area in Romania at 20 m resolution

    Examples:
        >>> from nrt import data

        >>> s2_cube = data.romania_20m()
        >>> # Compute NDVI
        >>> s2_cube['ndvi'] = (s2_cube.B8A - s2_cube.B4) / (s2_cube.B8A + s2_cube.B4)
        >>> # Filter clouds
        >>> s2_cube = s2_cube.where(s2_cube.SCL.isin([4,5,7]))
    """
    return _load('sentinel2_cube_subset_romania_20m.nc')


def romania_forest_cover_percentage():
    """Subset of Copernicus HR layer tree cover percentage - 20 m - Romania
    """
    file_basename = 'tree_cover_density_2018_romania.tif'
    filename = os.path.join(data_dir, file_basename)
    with rasterio.open(filename) as src:
        arr = src.read(1)
    return arr


def mre_crit_table():
    """Contains a dictionary equivalent to strucchange's ``mreCritValTable``
    The key 'sig_level' is a list of the available pre-computed significance
    (1-alpha) values.

    The other keys contain nested dictionaries, where the keys are the
    available relative window sizes (0.25, 0.5, 1), the second keys are the
    available periods (2, 4, 6, 8, 10) and the third keys are the functional
    types ("max", "range").

    Example:
        >>> from nrt import data
        >>> crit_table = data.mre_crit_table()
        >>> win_size = 0.5
        >>> period = 10
        >>> functional = "max"
        >>> alpha=0.025
        >>> crit_values = crit_table.get(str(win_size))\
                                    .get(str(period))\
                                    .get(functional)
        >>> sig_level = crit_table.get('sig_levels')
        >>> crit_level = np.interp(1-alpha, sig_level, crit_values)
    """
    with open(os.path.join(data_dir, "mreCritValTable.json")) as crit:
        crit_table = json.load(crit)
    return crit_table


def make_ts(dates, break_idx=None, intercept=0.7, amplitude=0.15, magnitude=0.25,
            recovery_time=1095, sigma_noise=0.02, n_outlier=3,
            outlier_value=-0.1, n_nan=3):
    """Simulate a harmonic time-series with optional breakpoint, noise and outliers

    The time-series is generated by adding;
    - an intercept/trend component which varies depending on the phase of the time-series
    (stable, recovery)
    - An annual seasonal component
    - Random noise drawn from a normal distribution (white noise)
    Optional outliers are then added to randomly chosen observation as well as ``np.Nan`` values.
    Note that the seasonal cycles simulation approach used here is rather simplistic,
    using a sinusoidal model and therefore assuming symetrical and regular behaviour
    around the peak of the simulated variable. Actual vegetation signal is often more
    asymetrical and irregular.

    Args:
        dates (array-like): List or array of dates (numpy.datetime64)
        break_idx (int): Breakpoint index in the date array provided. Defaults to
            ``None``, corresponding to a stable time-series
        intercept (float): Intercept of the time-series
        amplitude (float): Amplitude of the harmonic model (note that at every point
            of the time-series, the actual model amplitude is multiplied by the intercept
        magnitude (float): Break magnitude (always a drop in y value)
        recovery_time (int): Time (in days) to recover the initial intersect value
            following a break
        sigma_noise (float): Sigma value of the normal distribution (mean = 0) from which
            noise values are drawn
        n_outlier (int): Number of outliers randomly assigned to observations of the
            time-series
        outlier_value (float): Value to assign to outliers
        n_nan (int): Number of ``np.nan`` (no data) assigned to observations of the
            time-series

    Example:
        >>> from nrt import data
        >>> import numpy as np
        >>> import matplotlib.pyplot as plt

        >>> dates = np.arange('2018-01-01', '2022-06-15', dtype='datetime64[W]')
        >>> ts = data.make_ts(dates=dates, break_idx=30)

        >>> plt.plot(dates, ts)
        >>> plt.show()

    Returns:
        np.ndarray: Array of simulated values of same size as ``dates``
    """
    timestamps = (dates - np.datetime64(0, 'D')).astype(int)
    ydays = (dates - dates.astype('datetime64[Y]')).astype(int) + 1
    y = np.empty_like(dates, dtype=np.float64)
    # INtercept array
    y[:] = intercept
    # Build trend segment if break
    if break_idx:
        # Segment bounds
        segment_start_y = intercept - magnitude
        segment_start_timestamp = timestamps[break_idx]
        segment_end_timestamp = segment_start_timestamp + recovery_time
        segment_end_idx = np.abs(segment_end_timestamp - timestamps).argmin()
        # Compute y values
        recovery_rate = magnitude / recovery_time
        days_since_break = timestamps - segment_start_timestamp
        trend_segment = (recovery_rate * days_since_break + segment_start_y)[break_idx + 1:segment_end_idx + 1]
        # include into y
        y[break_idx + 1:segment_end_idx + 1] = trend_segment
    # Seasonality
    amplitude_values = amplitude * y
    season = amplitude * np.sin(2 * np.pi * timestamps / 365.25 - 2)
    # noise and outliers
    noise = np.random.normal(0, sigma_noise, dates.size)
    # Combine the 3 (trend, season, noise) components
    ts = y + season + noise
    # Add optional outliers and Nans
    outliers_idx = np.random.choice(np.arange(0, dates.size), size=n_outlier)
    nan_idx = np.random.choice(np.arange(0, dates.size), size=n_nan)
    ts[outliers_idx] = outlier_value
    ts[nan_idx] = np.nan
    return ts


def make_cube_parameters(shape=(100,100),
                         break_idx_interval=(0,100),
                         intercept_interval=(0.6, 0.8),
                         amplitude_interval=(0.12, 0.2),
                         magnitude_interval=(0.2, 0.3),
                         recovery_time_interval=(800,1400),
                         sigma_noise_interval=(0.02, 0.04),
                         n_outliers_interval=(0,5),
                         n_nan_interval=(0,5),
                         unstable_proportion=0.5):
    """Create ``xarray.Dataset`` of paramters for generation of synthetic data cube

    Prepares the main input required by the the ``make_cube`` function. This
    intermediary step eases the creation of multiple synthetic DataArrays sharing
    similar characteristics (e.g. to simulate multispectral data)

    Args:
        shape (tuple): A size two integer tuple giving the x,y size of the Dataset to be
            generated
        break_idx_interval (tuple): A tuple of two integers indicating the interval
            from which the breakpoint position in the time-series is drawn. Generate
            array of random values passed to the ``break_idx` argument of ``make_ts``.
            TODO: add a default to allow breakpoint at any location (conflict with Nan that indicate no break)
        intercept_interval (tuple): A tuple of two floats providing the interval
            from which intercept is drawn. Generate array of random values passed
            to the ``intercept`` argument of ``make_ts``
        amplitude_interval (tuple): A tuple of two floats indicating the interval
            from which the seasonal amplitude parameter is drawn. Generate array
            of random values passed to the ``amplitude`` argument of ``make_ts``
        magnitude_interval (tuple): A tuple of two floats indicating the interval
            from which the breakpoint magnitude parameter is drawn. Generate array
            of random values passed to the ``magnitude`` argument of ``make_ts``
        recovery_time_interval (tuple): A tuple of two integers indicating the interval
            from which the recovery time parameter (in days) is drawn. Generate array
            of random values passed to the ``recovery_time` argument of ``make_ts``
        sigma_noise_interval (tuple): A tuple of two floats indicating the interval
            from which the white noise level is drawn. Generate array of random
            values passed to the ``sigma_noise` argument of ``make_ts``
        n_outliers_interval (tuple): A tuple of two integers indicating the interval
            from which the number of outliers is drawn. Generate array
            of random values passed to the ``n_outliers` argument of ``make_ts``
        n_nan_interval (tuple): A tuple of two integers indicating the interval
            from which the number of no-data observations is drawn. Generate array
            of random values passed to the ``n_nan` argument of ``make_ts``
        unstable_proportion (float): Proportion of time-series containing a breakpoint.
            The other time-series are stable.

    Returns:
        xarray.Dataset: Dataset with arrays of parameters required for the generation
            of synthetic time-series using the spatialized version of ``make_ts``
            (see ``make_cube``)

    Examples:
        >>> import time
        >>> import numpy as np
        >>> from nrt import data
        >>> params_nir = data.make_cube_parameters(shape=(20,20),
        ...                                        n_outliers_interval=(0,0),
        ...                                        n_nan_interval=(0,0),
        ...                                        break_idx_interval=(100,200))
        >>> params_red = params_nir.copy(data={'intercept': np.random.uniform(0.09, 0.12, size=(20,20)),
        ...                                    'magnitude': np.random.uniform(-0.1, -0.03, size=(20,20)),
        ...                                    'amplitude': np.random.uniform(0.03, 0.07, size=(20,20))})
        >>> params_green = params_nir.copy(data={'intercept': np.random.uniform(0.12, 0.20, size=(20,20)),
        ...                                      'magnitude': np.random.uniform(0.05, 0.1, size=(20,20)),
        ...                                      'amplitude': np.random.uniform(0.05, 0.08, size=(20,20))})
        >>> params_blue = params_nir.copy(data={'intercept': np.random.uniform(0.08, 0.13, size=(20,20)),
        ...                                     'magnitude': np.random.uniform(-0.01, 0.01, size=(20,20)),
        ...                                     'amplitude': np.random.uniform(0.02, 0.04, size=(20,20))})
        >>> dates = np.arange('2018-01-01', '2022-06-15', dtype='datetime64[W]')
        >>> nir = data.make_cube(dates, name='nir', params_ds=params_nir)
        >>> red = data.make_cube(dates, name='red', params_ds=params_red)
        >>> green = data.make_cube(dates, name='green', params_ds=params_green)
        >>> blue = data.make_cube(dates, name='blue', params_ds=params_blue)
        >>> cube = xr.merge([blue, green, red, nir])
        >>> # PLot one ts
        >>> cube.isel(x=5, y=5).plot()
        >>> plt.show()
    """
    intercept = np.random.uniform(*intercept_interval, size=shape)
    amplitude = np.random.uniform(*amplitude_interval, size=shape)
    magnitude = np.random.uniform(*magnitude_interval, size=shape)
    recovery_time = np.random.randint(*recovery_time_interval, size=shape)
    sigma_noise = np.random.uniform(*sigma_noise_interval, size=shape)
    n_outlier = np.random.randint(*n_outliers_interval, size=shape)
    n_nan = np.random.randint(*n_nan_interval, size=shape)
    break_idx = np.random.randint(*break_idx_interval, size=shape)
    # Make a proportion of these cells stable
    size = np.multiply(*shape)
    stable_size = size - round(unstable_proportion * size)
    break_idx.ravel()[np.random.choice(size, stable_size, replace=False)] = np.nan
    # Build Dataset of parameters
    params = xr.Dataset(data_vars={'intercept': (['y', 'x'], intercept),
                                   'amplitude': (['y', 'x'], amplitude),
                                   'magnitude': (['y', 'x'], magnitude),
                                   'recovery_time': (['y', 'x'], recovery_time),
                                   'sigma_noise': (['y', 'x'], sigma_noise),
                                   'n_outlier': (['y', 'x'], n_outlier),
                                   'n_nan': (['y', 'x'], n_nan),
                                   'break_idx': (['y', 'x'], break_idx)},
                        coords={'y': np.arange(shape[0]),
                                'x': np.arange(shape[1])})
    return params


def make_cube(dates, name='ndvi', shape=(100,100),
              intercept_interval=(0.6, 0.8), amplitude_interval=(0.12, 0.2),
              magnitude_interval=(0.2, 0.3), recovery_time_interval=(800,1400),
              sigma_noise_interval=(0.02, 0.04), n_outliers_interval=(0,5),
              n_nan_interval=(0,5), break_daterange=None,
              unstable_proportion=0.5):
    """
    Args:
        dates (array-like): List or array of dates (numpy.datetime64)

    Example:
        >>> import time
        >>> from nrt import data
        >>> import matplotlib.pyplot as plt
        >>> # Compile
        >>> dates = np.arange('2018-01-01', '2022-06-15', dtype='datetime64[W]')
        >>> _ = make_cube(dates=dates, shape=(5,5))
        >>> # Time compiled execution of larger array
        >>> t0 = time.time()
        >>> params, cube = make_cube(dates=dates, shape=(200,200))
        >>> t1 = time.time()
        >>> print('Execution time: %.1f' % (t1 - t0))
        >>> print(cube)
        >>> print(params)
        >>> # PLot one ts
        >>> cube.isel(x=5, y=5).plot()
        >>> plt.show()
    """
    size = shape[0] * shape[1]
    intercepts = np.random.uniform(*intercept_interval, size=size)
    amplitudes = np.random.uniform(*amplitude_interval, size=size)
    magnitudes = np.random.uniform(*magnitude_interval, size=size)
    recovery_times = np.random.randint(*recovery_time_interval, size=size)
    sigma_noises = np.random.uniform(*sigma_noise_interval, size=size)
    n_outliers = np.random.randint(*n_outliers_interval, size=size)
    n_nans = np.random.randint(*n_nan_interval, size=size)
    # TODO: Compute min and max id of breaks if any break_daterange is provided
    break_idxx = np.random.randint(0, dates.size, size=size)
    # Create output array
    out_flat = np.empty((dates.size, size), dtype=np.float64)
    for i in range(out_flat.shape[1]):
        out_flat[:,i] = make_ts(dates=dates, break_idx=break_idxx[i])
    # Build xarray dataset
    xr_cube = xr.DataArray(data=out_flat.reshape((dates.size,*shape)),
                           coords={'time': dates, 'y': np.arange(shape[0]),
                                   'x': np.arange(shape[1])},
                          name=name)
    # Build Dataset of parameters
    params = xr.Dataset(data_vars={'intercept': (['y', 'x'], intercepts.reshape(shape)),
                                   'amplitude': (['y', 'x'], amplitudes.reshape(shape)),
                                   'magnitude': (['y', 'x'], magnitudes.reshape(shape)),
                                   'recovery_time': (['y', 'x'], recovery_times.reshape(shape)),
                                   'sigma_noise': (['y', 'x'], sigma_noises.reshape(shape)),
                                   'n_outlier': (['y', 'x'], n_outliers.reshape(shape)),
                                   'n_nan': (['y', 'x'], n_nans.reshape(shape)),
                                   'break_idx': (['y', 'x'], break_idxx.reshape(shape))},
                        coords={'y': np.arange(shape[0]),
                                'x': np.arange(shape[1])})
    return params, xr_cube


if __name__ == "__main__":
    import doctest
    doctest.testmod()
